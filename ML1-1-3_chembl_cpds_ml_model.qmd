---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.3 - Logistic regression model building in *Scikit-learn*
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

##### **Import libraries**

This post will need the following libraries to build and train a logistic regression model before using the model to predict max phase outcome on a testing dataset in scikit_learn.

```{python}
## using magic pip to install sklearn & matplotlib (somehow venv keeps switching off in vscode...)
# %pip install -U scikit-learn
# %pip install matplotlib


import sklearn
print(f"scikit-learn version used is: {sklearn.__version__}")
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import polars as pl
import matplotlib.pyplot as plt
```

The same set of data saved in the previous post will be read here using polars dataframe library.

```{python}
df = pl.read_csv("df_ml.csv")
df
```

<br>

##### **Logistic regression with *scikit-learn***

Logistic regression (LR) is one of the supervised methods in the statistical machine learning (ML) area. As the term "supervised" suggests, this type of ML is purely data-driven to allow computers to learn patterns from the input data with known outcomes in order to predict the same target outcomes for a different set of data that is previously unseen by the computer.

###### **Define X and y variables**

I'm using the `to_numpy()` directly in polars to convert data in the dataframe into Numpy arrays (saves from importing Numpy first and then converting into ndarrays).

```{python}
# Define X variables
X = df["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"]#.to_numpy()
X
```

```{python}
# df.columns[1:]
```

```{python}
# X_features = df.columns
# feat_names = pl.Series("feat_names", df.columns[1:])
# feat_names
```

```{python}
# type(feat_names)
```

```{python}
# Define y variable
y = df["Max_Phase"]#.to_numpy()
y
```

<br>

###### **Prepare training and testing sets**

The dataset is splitted into training and testing sets.

```{python}
## Random number generator
#rng = np.random.RandomState(0) - note: this may produce different result each time

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)
print('Training set:', X_train.shape, y_train.shape)
print('Testing set:', X_test.shape, y_test.shape)
```

<br>

###### **Pipeline method**

Some benefits of using pipeline ([scikit_learn reference](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators)):

- chaining preprocessing step with different estimators in one go where we only have to call fit and predict once on our data 

- avoiding data leakage from the testing set into the training set by making sure the same set of samples is used to train the transformers and predictors 

- avoiding missing out on the transformation step (note: calling `fit()` on pipeline is equivalent to calling `fit()` on each estimator and `transform()` input data before the next step)

The example below uses the function of `make_pipeline()` to construct a pipeline that takes in a standard scaler to scale data and also a LR estimator.

**work-in-progress for below**

* An example re. polars df with set_ouput for pipeline - this generates polars df as output 
- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.set_output

```{python}
# Tried the following with pipeline - didn't work as well as expected

## Likely need to use a columntransformer (in order to show feature names) - https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html
# from sklearn.compose import ColumnTransformer
# num_cols = ["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"]
# ct = ColumnTransformer(
#     ("numerical", num_cols),
#     verbose_feature_names_out=False,
#   )
# ct.set_output(transform="polars")
```

```{python}
## Pipeline:

from sklearn import set_config
set_config(transform_output="polars")

# params_lr = {
#   # solver for large dataset
#   "solver": "saga",
#   "random_state": 250
# }

LR = Pipeline(steps=[
  # Preprocess/normalise the dataset
  ("StandardScaler", StandardScaler().set_output(transform="polars")), 
  # Create an instance of LR classifier 
  ("LogR", LogisticRegression()) #**params_lr
]
  )

LR.set_output(transform="polars")
LR.fit(X_train, y_train)
LR.predict(X_test)
LR.score(X_test, y_test)
```

```{python}
# Figuring out how to best use set_output in polars 
# So far, best use case is to show the feature_names_in_ along with coef_ 
# Initial issue is the feature names are only shown as x0, x1, x2... not useful
# Likely because all the mol features are not in strings (either i64 or f64, so the feature names are not shown)


log_reg = LR[-1]
log_reg
```

```{python}
# type(log_reg)
# sklearn.linear_model._logistic.LogisticRegression
```

```{python}
# The other way to get features names is from pipeline only
# requires to save df column names separately as an array
# LR[:-1].get_feature_names_out(feat_names)

# the above pipeline works to show feature names - appears to need to set_output in 3 places - outside pipeline twice and inside pipeline once (for standard scaler part)
log_reg.feature_names_in_
```

```{python}
log_reg.coef_
```

```{python}
# Save array as df
lr_feat = pl.Series(log_reg.feature_names_in_).to_frame()
lr_feat
```

```{python}
# Need to un-pivot the dataframe
lr_coef = pl.Series(log_reg.coef_).to_frame()
lr_coef
```

```{python}
# Can plot feature names with coefficients

```


```{{python}}
## My example of a make_pipeline() used previously - note: this uses scikit_mol

Set parameters for xgboost model
params_xgboost = {
    "n_estimators": 100,
    "max_depth": 3,
    # For multi-class classification, use softprob for loss function (learning task parameters)
    # source: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
    "objective": 'multi:softprob', 
    "learning_rate": 0.1, 
    "subsample": 0.5, 
    "random_state": 2
    }

Building XGBoostClassifier pipeline
mlpipe_xgb = make_pipeline(
    # Convert SMILES to RDKit molecules
    SmilesToMolTransformer(), 
    # Molecule standardisations
    Standardizer(),
    ## A choice of using either Morgan fingerprints  or RDKit 2D descriptors:
    # Generate MorganFingerprintTransformer()
    MorganFingerprintTransformer(useFeatures=True),
    # Generate RDKit2D descriptors
    #MolecularDescriptorTransformer(**params_rdkit2d),
    # Scale variances in descriptor data
    StandardScaler(),
    # XGBoost classifier
    XGBClassifier(**params_xgboost)
)
```



**Converting predicted probabilities into a dataframe**

```{python}
# Use describe() to show distributions
# y_mp_prob = pd.DataFrame(y_mp_proba)
# y_mp_prob.describe()
```


**Converting predicted values into a dataframe**

```{python}
# Predicted values were based on log odds
# Use describe() method to get characteristics of the distribution
# pred = pd.DataFrame(LogR.predict_log_proba(X))
# pred.describe()
```

Alternatively, a quicker way to get predicted probabilities was via predict_proba() method in *scikit-learn*.

```{python}
# y_mp_proba = LogR.predict_proba(X_test)
# Uncomment below to see the predicted probabilities printed
#print(y_mp_proba)
```
