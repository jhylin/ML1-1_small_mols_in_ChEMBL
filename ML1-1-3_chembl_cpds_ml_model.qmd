---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.3 - Logistic regression model building in *Scikit-learn*
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

##### **Import libraries**

This post will need the following libraries to build and train a logistic regression model before using the model to predict max phase outcome on a testing dataset in scikit_learn.

```{python}
## using magic pip to install sklearn & matplotlib (somehow venv keeps switching off in vscode...)
# %pip install -U scikit-learn
# %pip install altair
# %pip install matplotlib (likely not going to use)


import sklearn
print(f"scikit-learn version used is: {sklearn.__version__}")
from sklearn import preprocessing, set_config
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler
import polars as pl
import matplotlib.pyplot as plt
```

The same set of data saved in the previous post will be read here using polars dataframe library.

```{python}
df = pl.read_csv("df_ml.csv")
df
```

<br>

##### **Logistic regression with *scikit-learn***

Logistic regression (LR) is one of the supervised methods in the statistical machine learning (ML) area. As the term "supervised" suggests, this type of ML is purely data-driven to allow computers to learn patterns from the input data with known outcomes in order to predict the same target outcomes for a different set of data that is previously unseen by the computer.

###### **Define X and y variables**

*not need to use to_numpy() as there's transform step included in LR estimator when being used in pipeline*

```{python}
# Define X variables
X = df["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"]#.to_numpy()
X
```

```{python}
# Define y variable
y = df["Max_Phase"]#.to_numpy()
y
```

<br>

###### **Prepare training and testing sets**

The dataset is splitted into training and testing sets.

```{python}
## Random number generator
#rng = np.random.RandomState(0) - note: this may produce different result each time

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)
print('Training set:', X_train.shape, y_train.shape)
print('Testing set:', X_test.shape, y_test.shape)
```

<br>

###### **Pipeline method**

Some benefits of using pipeline ([scikit_learn reference](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators)):

- chaining preprocessing step with different estimators in one go where we only have to call fit and predict once on our data 

- avoiding data leakage from the testing set into the training set by making sure the same set of samples is used to train the transformers and predictors 

- avoiding missing out on the transformation step (note: calling `fit()` on pipeline is equivalent to calling `fit()` on each estimator and `transform()` input data before the next step)

The example below uses `Pipeline()` to construct a pipeline that takes in a standard scaler to scale data and also a LR estimator, along with some parameters.

```{python}
## Pipeline:

set_config(transform_output="polars")

params_lr = {
  # solver for large dataset
  "solver": "saga",
  "random_state": 50
}

LR = Pipeline(steps=[
  # Preprocess/scale the dataset
  ("StandardScaler", StandardScaler().set_output(transform="polars")), 
  # Create an instance of LR classifier 
  ("LogR", LogisticRegression(**params_lr))
  ])

LR.set_output(transform="polars")
LR.fit(X_train, y_train)
LR.predict(X_test)
LR.score(X_test, y_test)
```

```{python}
# Figuring out how to best use set_output in polars 
# So far, best use case is to show the feature_names_in_ along with coef_ 
# Initial issue is the feature names are only shown as x0, x1, x2... not useful
# Likely because all the mol features are not in strings (either i64 or f64, so the feature names are not shown)

## Tried the following with pipeline to get polars df output - didn't work as well as expected
## Likely need to use a columntransformer (in order to show feature names) - https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html
# from sklearn.compose import ColumnTransformer
# num_cols = ["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"]
# ct = ColumnTransformer(
#     ("numerical", num_cols),
#     verbose_feature_names_out=False,
#   )
# ct.set_output(transform="polars")

# The other way to get features names is from pipeline only
# requires to save df column names separately as an array
# note: df.columns = feature/column names of physicochemical properties
# feat_names = pl.Series("feat_names", df.columns[1:])
# feat_names
# LR[:-1].get_feature_names_out(feat_names)

## the above pipeline works to show feature names - appears to need to set_output in 3 places - outside pipeline twice and inside pipeline once (for standard scaler part) - *will check if any of them is not necessary?*
```

I'm calling out the LR model used above in the pipeline as we want to get the feature names used for training and predicting along with their corresponding LR coefficients.

```{python}
log_reg = LR[-1]
log_reg
```

```{python}
# Save feature array as df
lr_feat = pl.Series(log_reg.feature_names_in_).to_frame("Feature_names")
# Explode df due to a list series - e.g. array([[1, 2, 3...]]) and not array([1, 2, 3...])
lr_coef = pl.Series(log_reg.coef_).to_frame("Coef").explode("Coef")
# Concatenate dfs horizontally
df_feat = pl.concat([lr_feat, lr_coef], how="horizontal")

# Requires altair to be installed
# Plotting feature names with coefficients 
df_feat.plot.bar(
  x="Coef", 
  y="Feature_names", 
  color="Feature_names",
  tooltip="Coef"
)

# How to enlarge chart?
```


*All needed below?*

**Converting predicted values into a dataframe**

```{python}
# Predicted values were based on log odds
# Use describe() to get stats
pred = pl.DataFrame(LR.predict_log_proba(X))
pred.describe()
```

Alternatively, a quicker way to get predicted probabilities is via predict_proba() method in *scikit-learn*.

```{python}
y_mp_proba = LR.predict_proba(X_test)
print(y_mp_proba)
```

**Converting predicted probabilities into a dataframe**

```{python}
y_mp_prob = pl.DataFrame(y_mp_proba)
y_mp_prob.describe()
```