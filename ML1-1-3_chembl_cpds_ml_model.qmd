---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.3 - Logistic regression model building in *Scikit-learn*
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

##### **Import libraries**

```{python}
## using magic pip to install sklearn & matplotlib (somehow venv keeps switching off in vscode...)
# %pip install -U scikit-learn
# %pip install matplotlib


import sklearn
print(f"scikit-learn version used is: {sklearn.__version__}")
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import polars as pl
import matplotlib.pyplot as plt
```

---old way using pandas---

```{python}
# I've then installed pyarrow, to convert Polars dataframe into a Pandas dataframe, which was needed to run *scikit-learn*.

# Uncomment line below to install pyarrow
#!pip install pyarrow

# Convert Polars df to Pandas df 
# df_ml_pd = df_ml.to_pandas()
# type(df_ml_pd)
```

---new way using polars---

```{python}
df = pl.read_csv("df_ml.csv")
df
```

##### **Logistic regression with *scikit-learn***

Logistic regression (LR) is one of the supervised methods in the statistical machine learning (ML) area. As the term "supervised" suggests, this type of ML is purely data-driven to allow computers to learn patterns from the input data with known outcomes in order to predict the same target outcomes for a different set of data that is previously unseen by the computer.

###### **Define X and y variables**

```{python}
# Define X variables from df dataset
X = df["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"].to_numpy()
X
```

```{python}
# Define y variable
y = df["Max_Phase"].to_numpy()
y
```

<br>

###### **Prepare training and testing sets**

The dataset is splitted into training and testing sets.

```{python}
## Random number generator
#rng = np.random.RandomState(0) - note: this may produce different result each time

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)
print('Training set:', X_train.shape, y_train.shape)
print('Testing set:', X_test.shape, y_test.shape)
```

<br>

###### **Pipeline method**

Some benefits of using pipeline ([scikit_learn reference](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators)):

- chaining preprocessing step with different estimators in one go where we only have to call fit and predict once on our data 

- avoiding data leakage from the testing set into the training set by making sure the same set of samples is used to train the transformers and predictors 

- avoiding missing out on the transformation step (note: calling `fit()` on pipeline is equivalent to calling `fit()` on each estimator and `transform()` input data before the next step)

The example below uses the function of `make_pipeline()` to construct a pipeline that takes in a standard scaler to scale data and also a LR estimator.

**work-in-progress for below**

```{python}
# set_output() example in polars from sklearn

import polars as pl
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

df = pl.DataFrame(
    {"height": [120, 140, 150, 110, 100], "pet": ["dog", "cat", "dog", "cat", "cat"]}
)
preprocessor = ColumnTransformer(
    [
        ("numerical", StandardScaler(), ["height"]),
        ("categorical", OneHotEncoder(sparse_output=False), ["pet"]),
    ],
    verbose_feature_names_out=False,
)
preprocessor.set_output(transform="polars")

df_out = preprocessor.fit_transform(df)
df_out
```

* Another example re. polars df with set_ouput for pipeline - this generates polars df as output 
- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.set_output

```{python}
## Pipeline:
from sklearn import set_config
set_config(transform_output="polars")

## Likely need to use a columntransformer (in order to show feature names) - https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html

# params_lr = {
#   # solver for large dataset
#   "solver": "saga",
#   "random_state": 250
# }

LR = make_pipeline(
  # Preprocess/normalise the dataset
  StandardScaler().set_output(transform="polars"), 
  # Create an instance of LR classifier 
  LogisticRegression(), #**params_lr
  )

LR.fit(X_train, y_train)
LR.predict(X_test)
#LR.score(X_test, y_test)
```

```{python}
# Figuring out how to best use set_output in polars 
# So far, best use case is to show the feature_names_in_ along with coef_ 
# issue is the feature names are only shown as x0, x1, x2... not useful

log_reg = LR[-1]
log_reg
```

```{python}
type(log_reg)
# sklearn.linear_model._logistic.LogisticRegression
```

```{python}
# Likely because all the mol features are not in strings (either i64 or f64, so the feature names are not shown)
log_reg.feature_names_in_
```

```{python}
# Save array as df
lr = pl.Series(log_reg.feature_names_in_).to_frame()
lr
```

```{python}
log_reg.coef_
```


```{{python}}
## My example of a make_pipeline() used previously - note: this uses scikit_mol

Set parameters for xgboost model
params_xgboost = {
    "n_estimators": 100,
    "max_depth": 3,
    # For multi-class classification, use softprob for loss function (learning task parameters)
    # source: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
    "objective": 'multi:softprob', 
    "learning_rate": 0.1, 
    "subsample": 0.5, 
    "random_state": 2
    }

Building XGBoostClassifier pipeline
mlpipe_xgb = make_pipeline(
    # Convert SMILES to RDKit molecules
    SmilesToMolTransformer(), 
    # Molecule standardisations
    Standardizer(),
    ## A choice of using either Morgan fingerprints  or RDKit 2D descriptors:
    # Generate MorganFingerprintTransformer()
    MorganFingerprintTransformer(useFeatures=True),
    # Generate RDKit2D descriptors
    #MolecularDescriptorTransformer(**params_rdkit2d),
    # Scale variances in descriptor data
    StandardScaler(),
    # XGBoost classifier
    XGBClassifier(**params_xgboost)
)
```



**Converting predicted probabilities into a dataframe**

```{python}
# Use describe() to show distributions
# y_mp_prob = pd.DataFrame(y_mp_proba)
# y_mp_prob.describe()
```


**Converting predicted values into a dataframe**

```{python}
# Predicted values were based on log odds
# Use describe() method to get characteristics of the distribution
# pred = pd.DataFrame(LogR.predict_log_proba(X))
# pred.describe()
```

Alternatively, a quicker way to get predicted probabilities was via predict_proba() method in *scikit-learn*.

```{python}
# y_mp_proba = LogR.predict_proba(X_test)
# Uncomment below to see the predicted probabilities printed
#print(y_mp_proba)
```
