---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.4 - Evaluating logistic regression model in *Scikit-learn*
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

##### **Import libraries**

```{python}
import sklearn
print(f"scikit-learn version used is: {sklearn.__version__}")
from sklearn.model_selection import train_test_split
import polars as pl
print(f"polars version used is: {pl.__version__}")
import pickle
# import numpy as np
import matplotlib.pyplot as plt
# For accuracy scores, confusion matrix, ROC curve, classification report & log loss
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay, roc_curve, log_loss
```

<br>

##### **Import logistic regression pipeline/model**

Loading the pickled file here first.

```{python}
LR = pickle.load(open("LR.pkl", "rb"))
LR
```

<br>

##### **Evaluations of the logistic regression model**

Outline:
* Accuracy scores
* Confusion matrix
* ROC curve (to be added)
* Classification report (precision, recall, f1-score, support)
* Log loss

###### **Accuracy scores**

```{python}
## Read in data & split into training & testing sets
df = pl.read_csv("df_ml.csv")
X = df["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"]
y = df["Max_Phase"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)
LR.fit(X_train, y_train)
y_mp = LR.predict(X_test)

# y_mp = predicted y 
# y_test = actual/true y
accuracy_score(y_mp, y_test)

## getting a warning message after accuracy score generated earlier - "UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names" - resolved, missed the fitting step first (my bad) before predict()
```

The accuracy score is 0.69 (after rounding up) which means that there are around 70% of the cases or compounds classified correctly by using the LR classifier. This score is also the same as the one shown from the previous post using `score()` instead of `accuracy_score()`.

Accuracy score gives an idea about how close the predicted samples are to the true values. One caveat to note is that for imbalanced dataset, accuracy score might not be very informative and other evaluation metrics will be needed as well.

<br>

###### **Confusion matrix**

A confusion matrix is built below based on the model in order to visualise the counts of correct and incorrect predictions. Previous code used to plot confusion matrix is shown below:

```{python}
#| code-fold: true

# Function to print and plot confusion matrix
# The function code below was adapted from the IBM data science course I've taken previously

# Import itertools - to create iterators for efficient looping
# import itertools

# def plot_confusion_matrix(# Sets a cm object (cm = confusion matrix)
#                           cm, 
#                           # Sets classes of '1s' (Successes) & '0s' (Non-successes) for the cm
#                           classes,
#                           # If setting normalize = true, reports in ratios instead of counts
#                           normalize,
#                           title = 'Confusion matrix',
#                           # Choose colour of the cm (using colourmap recognised by matplotlib)
#                           cmap = plt.cm.Reds):
    
#     if normalize:
#         cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]
#         print("Normalized confusion matrix")
#     else:
#         print('Confusion matrix, without normalization')

#     print(cm)

#     # Plot the confusion matrix 
#     plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
#     plt.title(title)
#     plt.colorbar()
#     tick_marks = np.arange(len(classes))
#     plt.xticks(tick_marks, classes, rotation = 45)
#     plt.yticks(tick_marks, classes)

#     # Floats to be round up to two decimal places if using normalize = True
#     # or else use integers
#     fmt = '.2f' if normalize else 'd'
#     # Sets threshold of 0.5
#     thresh = cm.max() / 2.
#     # Iterate through the results and differentiate between two text colours 
#     # by using the threshold as a cut-off
#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
#         plt.text(j, i, format(cm[i, j], fmt),
#                  horizontalalignment = "center",
#                  color = "white" if cm[i, j] > thresh else "black")

#     plt.tight_layout()
#     plt.ylabel('True label')
#     plt.xlabel('Predicted label')
# ```

# ```{python}
# # Compute confusion matrix
# matrix = confusion_matrix(y_test, y_mp, labels = [0,1])
# np.set_printoptions(precision = 2)

# # Plot confusion matrix without normalisation
# plt.figure()
# plot_confusion_matrix(matrix, 
#                       # Define classes of outcomes
#                       classes = ['Max_Phase = 0','Max_Phase = 1'], 
#                       # Set normalize = True if wanting ratios instead
#                       normalize = False, 
#                       title = "Confusion matrix without normalisation"
#                      )
```

*Old post that's got roc curve & confusion matrix - https://jhylin.github.io/Data_in_life_blog/posts/17_ML2-2_Random_forest/2_random_forest_classifier.html*

There is actually an alternative and probably a better way (that uses less code) to plot confusion matrix using scikit_learn's code as shown below:

```{python}
ConfusionMatrixDisplay.from_estimator(LR, X_test, y_test)
plt.show()
```

A common rule of thumb for confusion matrix is that all predicted outcomes are columns and all the true outcomes are rows. However, there might be exceptions where this would be the other way round. 

Four different categories can be seen in the confusion matrix:

-   True positive - Predicted Max_Phase = 1 & True Max_Phase = 1 (391) - *what we're interested in*
-   True negative - Predicted Max_Phase = 0 & True Max_Phase = 0 (391)
-   False positive - Predicted Max_Phase = 1 & True Max_Phase = 0 (167)
-   False negative - Predicted Max_Phase = 0 & True Max_Phase = 1 (185)

<br>

###### **ROC curve**

**the following needs to be updated**

old post ref: https://jhylin.github.io/Data_in_life_blog/posts/17_ML2-2_Random_forest/2_random_forest_classifier.html#plotting-roc-curves

to be added.

<br>

###### **Classification report**

```{python}
print(classification_report(y_test, y_mp))
```

***Precision*** was a measure of the accuracy of a predicted outcome, where a class label had been predicted by the classifier. So in this case, we could see that for class label 1, the precision was 0.67, which corresponded to the true positive result of 126 out of 189 samples (= 0.666). It was defined by:

$$
\text{Precision} = \frac{\Sigma\ True\ Positive}{(\Sigma\ True\ Positive + \Sigma\ False\ Positive)}
$$

***Recall***, also known as sensitivity (especially widely used in biostatistics and medical diagnostic fields), was a measure of the strength of the classifier to predict a positive outcome. In simple words, it measured the true positive rate. In this example, there was a total of 126 out of 177 samples (which = 0.712, for True Max_Phase = 1 row) that had a true positive outcome of having a max phase of 1. It was defined by:

$$
\text{Recall} = \frac{\Sigma\ True\ Positive}{(\Sigma\ True\ Positive + \Sigma\ False\ Negative)}
$$

The precision and recall metrics could also be calculated for class label = 0, which were shown for the row 0 in the classification report.

***f1-score***, or also known as balanced F-score or F-measure, denoted the harmonic average of both precision and recall metrics. This metric would also give another indication about whether this model performed well on outcome predictions. It normally ranged from 0 (worst precision and recall) to 1 (perfect precision and recall). For this particular classifier, f1-score was at 0.69 (for class label = 1), which was definitely not at its worst, but also could be further improved. It was defined as:

$$
\text{F1-score} = \frac{2 \times (Precision \times Recall)}{(Precision + Recall)}
$$

***Support***, which some readers might have already worked out how the numbers were derived, was the total number of true samples in each class label (reading row-wise from the confusion matrix). The main purpose of showing this metric was to help clarifying whether the model or classifier had a reasonably balanced dataset for each class or otherwise.

<br>

###### **Log loss**

Log loss could be used as another gauge to show how good the classifier was at making the outcome predictions. The further the predicted probability was from the true value, the larger the log loss, which was also ranged from 0 to 1. Ideally, the smaller the log loss the better the model would be. Here, we had a log loss of 0.607 for this particular model.

```{python}
# Log loss

# log_loss(y_test, y_mp_proba)
```

#### **Discussions and conclusion**

So here I've completed a very basic LR classifier model for ChEMBL compound dataset. By no means was this a perfect ML model as I haven't actually changed the default settings of *scikit-learn*'s LogisticRegression() classifier, with examples such as adjusting C, a regularization parameter which was set at '1.0' by default, and also solvers, which could take in different algorithms for use in optimisation problems and normally set as 'lbfgs' by default.

So with this default LR model, the evaluation metrics demonstrated a LR classifer of moderate quality to predict the approval outcomes on ChEMBL small molecules, with a lot of rooms for improvements. Therefore, I could not yet confirm fully that the physicochemical parameters chosen would be the best ones to predict the approval outcomes for any small molecules. However, I might be okay to say that these molecular parameters were on the right track to help with making this prediction.

To further improve this model, I could possibly trial changing the C value and use different solvers to see if better outcomes could be achieved, or even add more molecular parameters in the model to test. I could have also trialled adding more class labels, e.g. making it between max phase 1, 2 and 4, or a mix-and-match between each max phase category. Other things to consider would be to use other types of ML methods such as naive Bayes, K-nearest neighbours or decision trees and so on. To tackle the problem thoroughly, I would most likely need to do an ensemble of different ML models to find out which model would be the most optimal to answer our target question.

<br>

#### **Final words**

I've experienced the fun of ML after completing this project. The idea was to build on what I knew gradually and enjoy what ML could do when making critical decisions. From what I've learnt about ML so far (and definitely more to learn) was that the quality of data was vital for making meaningful interpretations of the results.

However, jumping back to present time, I'll need to work on my second project first, which is about using Rust interactively via Jupyter notebook. At the moment, I'm not sure how long it will take or how the content will play out. I'll certainly do as much as I can since Rust is very new to me. If I get very stuck, I'd most likely continue on this ML series. Thanks for reading.

<br>

#### **References**

I've listed below most of the references used throughout this project. Again, huge thanks could not be forgotten for our online communities, and definitely also towards the references I've used here.

-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html)
-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.
-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.
-   [Stack Overflow](https://stackoverflow.com)
-   Polars references:
    1.  [Polars - User Guide](https://pola-rs.github.io/polars-book/user-guide/introduction.html) - https://pola-rs.github.io/polars-book/user-guide/introduction.html
    2.  [Polars documentation](https://pola-rs.github.io/polars/py-polars/html/index.html#) - https://pola-rs.github.io/polars/py-polars/html/index.html#
    3.  [Polars GitHub repository](https://github.com/pola-rs/polars) - https://github.com/pola-rs/polars
