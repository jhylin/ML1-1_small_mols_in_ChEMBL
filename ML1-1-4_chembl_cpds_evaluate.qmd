---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.4 - Evaluating logistic regression model in *Scikit-learn*
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

##### **Import libraries**

```{python}
import sklearn
print(f"scikit-learn version used is: {sklearn.__version__}")
from sklearn.model_selection import train_test_split
import polars as pl
print(f"polars version used is: {pl.__version__}")
import pickle
import matplotlib.pyplot as plt
from sklearn import metrics
# For accuracy scores, confusion matrix, ROC curve, classification report & log loss
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay, roc_curve, log_loss
```

<br>

##### **Import logistic regression pipeline/model**

Loading the pickled file here first.

```{python}
LR = pickle.load(open("LR.pkl", "rb"))
LR
```

<br>

##### **Evaluations of the logistic regression model**

This part will involve using accuracy scores, confusion matrix, ROC curve, classification report and log loss to evaluate the LR model.

###### **Accuracy scores**

```{python}
## Read in data & split into training & testing sets
df = pl.read_csv("df_ml.csv")
X = df["#RO5 Violations", "Polar Surface Area", "HBA", "HBD", "QED Weighted", "CX LogP", "CX LogD", "Heavy Atoms"]
y = df["Max_Phase"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)
LR.fit(X_train, y_train)
y_mp = LR.predict(X_test)

# y_mp = predicted y 
# y_test = actual/true y
accuracy_score(y_mp, y_test)

## getting a warning message after accuracy score generated earlier - "UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names" - resolved, missed the fitting step first (my bad) before predict()
```

The accuracy score is 0.69 (after rounding up) which means that there are around 70% of the cases or compounds classified correctly by using the LR classifier. This score is also the same as the one shown from the previous post using `score()` instead of `accuracy_score()`.

Accuracy score gives an idea about how close the predicted samples are to the true values. One caveat to note is that for imbalanced dataset, accuracy score might not be very informative and other evaluation metrics will be needed as well.

<br>

###### **Confusion matrix**

A confusion matrix is built below based on the model in order to visualise the counts of correct and incorrect predictions. Previous code used to plot confusion matrix is shown below:

```{python}
#| code-fold: true

## Function to print and plot confusion matrix
## The function code below was adapted from the IBM data science course I've taken previously

# # to create iterators for efficient looping
# import itertools
# import numpy as np

# def plot_confusion_matrix(# Sets a cm object (cm = confusion matrix)
#                           cm, 
#                           # Sets classes of '1s' (Successes) & '0s' (Non-successes) for the cm
#                           classes,
#                           # If setting normalize = true, reports in ratios instead of counts
#                           normalize,
#                           title = 'Confusion matrix',
#                           # Choose colour of the cm (using colourmap recognised by matplotlib)
#                           cmap = plt.cm.Reds):
    
    # if normalize:
    #     cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]
    #     print("Normalized confusion matrix")
    # else:
    #     print('Confusion matrix, without normalization')

    # print(cm)

    # # Plot the confusion matrix 
    # plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
    # plt.title(title)
    # plt.colorbar()
    # tick_marks = np.arange(len(classes))
    # plt.xticks(tick_marks, classes, rotation = 45)
    # plt.yticks(tick_marks, classes)

    # # Floats to be round up to two decimal places if using normalize = True
    # # or else use integers
    # fmt = '.2f' if normalize else 'd'
    # # Sets threshold of 0.5
    # thresh = cm.max() / 2.
    # # Iterate through the results and differentiate between two text colours 
    # # by using the threshold as a cut-off
    # for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    #     plt.text(j, i, format(cm[i, j], fmt),
    #              horizontalalignment = "center",
    #              color = "white" if cm[i, j] > thresh else "black")

    # plt.tight_layout()
    # plt.ylabel('True label')
    # plt.xlabel('Predicted label')

# # Compute confusion matrix
# matrix = confusion_matrix(y_test, y_mp, labels = [0,1])
# np.set_printoptions(precision = 2)

# # Plot confusion matrix without normalisation
# plt.figure()
# plot_confusion_matrix(matrix, 
#                       # Define classes of outcomes
#                       classes = ['Max_Phase = 0','Max_Phase = 1'], 
#                       # Set normalize = True if wanting ratios instead
#                       normalize = False, 
#                       title = "Confusion matrix without normalisation"
#                      )
```

There is actually an alternative and probably a better way (that uses less code) to plot confusion matrix using scikit_learn's code as shown below:

```{python}
ConfusionMatrixDisplay.from_estimator(LR, X_test, y_test)
plt.show()
```

A common rule of thumb for confusion matrix is that all predicted outcomes are columns and all the true outcomes are rows. However, there might be exceptions where this would be the other way round. 

Four different categories can be seen in the confusion matrix:

-   True positive - Predicted Max_Phase = 1 & True Max_Phase = 1 (391) - *interested in this*
-   True negative - Predicted Max_Phase = 0 & True Max_Phase = 0 (391)
-   False positive - Predicted Max_Phase = 1 & True Max_Phase = 0 (167)
-   False negative - Predicted Max_Phase = 0 & True Max_Phase = 1 (185)

<br>

###### **Receiver operating characteristic (ROC) curve**

My [old post](https://jhylin.github.io/Data_in_life_blog/posts/17_ML2-2_Random_forest/2_random_forest_classifier.html#brief-introduction) about random forest classifier has already explained what an area under the ROC curve is (I'm just going to quote myself...):

> Area under the ROC curve: [reference](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) - the area under a curve plot between sensitivity or recall (percent of all 1s classified correctly by a classifier or true positive rate) and specificity (percent of all 0s classified correctly by a classifier, or equivalent to 1 - false positive rate or true negative rate) (Bruce, Bruce, and Gedeck 2020). It is useful for evaluating the performance of a classification model via comparing the true positive rate and false positive rate which are influenced by shifting the decision threshold. Area under the ROC is usually represented as a number ranging from 0 to 1 (1 being a perfect classifier, 0.5 or below meaning a poor, ineffective classifier)

In this case, we can also apply ROC curve to the LR model and its predicted outcomes.

```{python}
# get the predicted probabilities of outcome = 1 (approved drugs)
y_mp_probs = LR.predict_proba(X_test)[:, 1]
RocCurveDisplay.from_predictions(y_test, y_mp_probs, plot_chance_level = True)
```

<br>

###### **Classification report**

```{python}
print(classification_report(y_test, y_mp))
```

***Precision*** is a measure of the accuracy of a predicted outcome, where a class label has been predicted by the classifier. In this case, we can see that for class label 1, the precision is 0.70, which corresponds to the true positive result of 391 out of 558 samples (= 0.70, for true predicted Max_Phase = 1 column). It is defined by:

$$
\text{Precision} = \frac{\Sigma\ True\ Positive}{(\Sigma\ True\ Positive + \Sigma\ False\ Positive)}
$$

***Recall***, also known as sensitivity (especially widely used in biostatistics and medical diagnostic fields), is a measure of the strength of the classifier to predict a positive outcome. In simple words, it measures the true positive rate. In this example, there is a total of 391 out of 576 samples (which = 0.68, for true Max_Phase = 1 row). It is defined by:

$$
\text{Recall} = \frac{\Sigma\ True\ Positive}{(\Sigma\ True\ Positive + \Sigma\ False\ Negative)}
$$

The precision and recall metrics are also calculated and shown for row 0 in the classification report.

***f1-score***, or also known as balanced F-score or F-measure, denoted the harmonic average of both precision and recall metrics. This metric will also give another indication about whether this model performed well on outcome predictions. Its range is normally from 0 (worst precision and recall) to 1 (perfect precision and recall). For this particular classifier, f1-score was at 0.69 (for class label = 1). It is defined as:

$$
\text{F1-score} = \frac{2 \times (Precision \times Recall)}{(Precision + Recall)}
$$

***Support*** is the total number of true samples in each class label (reading row-wise from the confusion matrix). The main purpose of showing this metric is to help clarifying whether the model has had a reasonably balanced dataset for each class (and also helped to check the precision and recall values if needed).

<br>

###### **Log loss**

Log loss can be used as another metric to show how good the classifier is at making the predictions. The further apart the predicted probability is from the true value, the larger the log loss, which is also ranging from 0 to 1. Ideally, the smaller the log loss the better the model will be. Here, we have a log loss of 0.607 for this particular model.

```{python}
log_loss(y_test, y_mp_probs)
```

<br>

#### **Discussions and conclusion**

**the following needs to be updated**

So here I've completed a very basic LR classifier model for ChEMBL compound dataset. By no means was this a perfect ML model as I haven't actually changed the default settings of *scikit-learn*'s LogisticRegression() classifier, with examples such as adjusting C, a regularization parameter which was set at '1.0' by default, and also solvers, which could take in different algorithms for use in optimisation problems and normally set as 'lbfgs' by default.

So with this default LR model, the evaluation metrics demonstrated a LR classifer of moderate quality to predict the approval outcomes on ChEMBL small molecules, with a lot of rooms for improvements. Therefore, I could not yet confirm fully that the physicochemical parameters chosen would be the best ones to predict the approval outcomes for any small molecules. However, I might be okay to say that these molecular parameters were on the right track to help with making this prediction.

To further improve this model, I could possibly trial changing the C value and use different solvers to see if better outcomes could be achieved, or even add more molecular parameters in the model to test. I could have also trialled adding more class labels, e.g. making it between max phase 1, 2 and 4, or a mix-and-match between each max phase category. Other things to consider would be to use other types of ML methods such as naive Bayes, K-nearest neighbours or decision trees and so on. To tackle the problem thoroughly, I would most likely need to do an ensemble of different ML models to find out which model would be the most optimal to answer our target question.

<br>

#### **Final words**

I've experienced the fun of ML after completing this project. The idea was to build on what I knew gradually and enjoy what ML could do when making critical decisions. From what I've learnt about ML so far (and definitely more to learn) was that the quality of data was vital for making meaningful interpretations of the results.

However, jumping back to present time, I'll need to work on my second project first, which is about using Rust interactively via Jupyter notebook. At the moment, I'm not sure how long it will take or how the content will play out. I'll certainly do as much as I can since Rust is very new to me. If I get very stuck, I'd most likely continue on this ML series. Thanks for reading.

<br>

#### **References**

I've listed below most of the references used throughout this project. Again, huge thanks could not be forgotten for our online communities, and definitely also towards the references I've used here.

-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html)
-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.
-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.
-   [Stack Overflow](https://stackoverflow.com)
-   Polars references:
    1.  [Polars - User Guide](https://pola-rs.github.io/polars-book/user-guide/introduction.html) - https://pola-rs.github.io/polars-book/user-guide/introduction.html
    2.  [Polars documentation](https://pola-rs.github.io/polars/py-polars/html/index.html#) - https://pola-rs.github.io/polars/py-polars/html/index.html#
    3.  [Polars GitHub repository](https://github.com/pola-rs/polars) - https://github.com/pola-rs/polars
