---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.1 - Polars dataframe library and parquet files
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

**An attempt to revamp this post - starting from 31/8/24**

Why revamping this post?

Three reasons for this: 

1. ChEMBL file downloaded straight from webiste was way too large to be uploaded to GitHub (this was one of my very early posts with ChEMBL being completely new to me at the time so I downloaded the data from ChEMBL website without thinking too much, obviously there are currently other better and more reproducible ways to source ChEMBL data e.g. my more recent posts)

Note: GitHub blocks files larger than 100MB (may need to use Git LFS instead)

2. Polars seems to be a bit more integrated with sklearn now so I'm wondering if Polars can be used with sklearn wholly (i.e. not using Pandas at all)

3. This post is one of my earlier less mature posts (bit embarrassing when I'm looking at it now) so I just want to see if I can improve it a little

<br>

**Plan**

1. Convert ChEMBL csv file into parquet file (which can be read using Polars and can be uploaded into GitHub)

2. Try to see if Polars can be used with sklearn completely for building ML model

<br>

##### ***Machine learning in drug discovery - series 1.1***

<br>

*Latest update from 19th April 2024 - Polars is currently more integrated with Scikit-learn from version 1.4 (since January 2024), see this link re. [Polars output in set_output](https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_4_0.html#polars-output-in-set-output) for Polars dataframe outputs in Scikit-learn, and also a few other Polars enhancements from release [version 1.4 changelog](https://scikit-learn.org/dev/whats_new/v1.4.html#). I may attempt to update this series of posts in the future as they most probably won't need the conversion from Polars to Pandas dataframes anymore.*

*Previous post update was on 16th August 2023 - some code updates only, please always refer to [Polars API reference](https://docs.pola.rs/py-polars/html/reference/index.html) documentations for most up-to-date code.*

<br>

##### **Background**

**to update**

As my interests gradually grew for Rust, I realised why so many people said it might be a hard programming language to learn. My head was spinning after reading the Rust programming language book and watching a few online teaching videos about it. I then decided to start from something I was more familiar with, and somehow through various online ventures and searching, I've managed to start two projects in parallel. The first one was where I used Polars dataframe library, and the second one would be about using Rust through an interactive user interface such as Jupyter notebook. I've anticipated that the second project would take much longer time for me to finish, so I would be tackling the first project for now.

This project was about using Polars, a blazingly fast dataframe library that was written completely in Rust with a very light Python binding that was available for use via Python or Rust, so I started using Polars via Python on Jupyter Lab initially, which involved data wrangling, some exploratory data analysis (EDA), and a reasonably larger section on using machine learning (ML) through *scikit-learn*. The editing and publishing of this post was mainly achieved via RStudio IDE.

<br>

##### **Install Polars**

```{python}
# To install Polars dataframe library
# Uncomment below to download and install Polars
#!pip install polars

# Update Polars version
# Uncomment the line below to update Polars
#!pip install --upgrade polars
```

Once Polars was installed, the next step was to import it for use.

```{python}
import polars as pl
```

```{python}
# Show version of Polars
# Uncomment line below to check version of Polars
pl.show_versions()
```

<br>

##### **Download dataset**

**to update**

The dataset, which was purely about small molecules and their physicochemical properties, was downloaded from [ChEMBL database](https://www.ebi.ac.uk/chembl/) and saved as a .csv file. I've decided not to upload the "chembl_mols.csv" file due to its sheer size (around 0.6 GB), and also I'd like to stay using free open-source resources (including GitHub) at this stage. I've looked into the Git large file system, but for the free version it only provides 2 GB, which at this stage, I think by adding this larger than usual .csv file along with my portfolio blog repository may exceed this limit in no time.

For anyone who would like to use the same dataset, the file I used would be equivalent to a straight download from the home page of ChEMBL database, via clicking on the "Distinct compounds" (please see the circled area in the image below). Options were available to download the files as .csv, .tsv or .sdf formats (located at the top right of the page).

<br>

![Image adapted from ChEMBL database website](ChEMBL_cpds.jpg){fig-align="center"}

<br>

Once we've had the file ready, it would be read via the usual read_csv() method.

```{python}
df = pl.read_csv("chembl_mols.csv")
df.head()
#df 
```

<br>

##### **Data wrangling and parquet files**

Now, since this dataset was downloaded as a .csv file, this meant it was likely to have a certain delimiter between each variable. So the whole dataset was presented as strings where each string represented each compound in each row. Each variable was separated by semicolons. To read it properly, I've added a delimiter term in the code to transform the dataframe into a more readable format.

```{python}
# By referring to Polars documentation, 
# *use "sep" to set the delimiter of the file
# which was semicolons in this case
# *please note this has been updated to "separator" 
# due to updates in Polars since the published date of this post
df = pl.read_csv("chembl_mols.csv", separator = ";")

#df.head(10)
df
```

```{python}
df.group_by("Type").len()
```

df is further reduced in size by restricting to only small molecules.

```{python}
df_sm = df.filter((pl.col("Type") == "Small molecule"))
df_sm #1,920,366 entries
```

```{python}
# df_sm.group_by("Structure Type").len()
```

There are 5485 entries with "NONE" as "Structure Type" which mean unknown compound structures or not recorded in either compound_structures table or protein_therapeutics table - will try to remove them from df_sm first.

```{python}
# Check "Inorganic Flag" if 1 meaning inorganic
df_sm.group_by("Inorganic Flag").len()

# 12,932 as "0" - unassigned inorganic flag
# 130 as "1" - inorganics
```

```{python}
# Reducing file size by filtering out 
# non-small molecules & structure type of "NONE"
# then try the default write_parquet() with comp level = 22

df_sm = df.filter((pl.col("Type") == "Small molecule") & (pl.col("Structure Type") != "NONE"))

df_sm #1,914,881 entries
```

```{python}
# Check "NONE" entries are removed/filtered
df_sm.group_by("Structure Type").len()
```

```{python}
# Adding in inorganic flag
df_sm = df.filter((pl.col("Type") == "Small molecule") & (pl.col("Structure Type") != "NONE") & (pl.col("Inorganic Flag") == -1 ))

df_sm #1,901,819 entries
```

```{python}
from pathlib import Path
```

```{python}
# Convert df into parquet file
path = Path.cwd() / "chembl_sm_mols.parquet"
df_sm.write_parquet(path, compression_level=22)

## Check parquet file size
# file size when comp level at 22 = 122.1MB (original ~2 million entries)

# file size when comp level at 22 = 100.8MB when file restricted by "Type" == "Small molecule" (1,920,366 entries)

# file size when comp level at 22 = 100.4MB when file restricted by "Type" == "Small molecule" & "Structure Type" != "NONE" (1,914,881 entries)

# file size when comp level at 22 = 98.9MB when file restricted by "Type" == "Small molecule" & "Structure Type" != "NONE" & "Inorganic Flag" == -1 (1,901,819 entries entries)
```

```{python}
# try using use_pyarrow & pyarrow_options to partition datasets

# path = Path.cwd() / "chembl_mols_type_part_obj"
# df.write_parquet(
#     path,
#     use_pyarrow=True,
#     pyarrow_options={"partition_cols": ["Type"]},
# )

## when comp level as default (not specified)
# file sizes for max phase 1, 2, 3, 4 are all < 100MB
# file size for max phase 0 is > 100MB

## when comp level is 15
# one extra file was produced for each max phase category - max phase 0 had two > 100MB files (not ideal)

## when comp level is 20
# 3 files created for each max phase category of similar size as mentioned previously

## If using "Type" as partition column - "Small molecule" parquet file size = 134.3MB
```
