---
title: Small molecules in ChEMBL database
subtitle: Series 1.1 - Polars dataframe library and machine learning in *scikit-learn*
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

```{python}
# read parquet files as df

```

**The following needs to be updated & splitted into smaller posts/qmds**

Initially, I only wanted to download around 24 compounds from the ChEMBL database first. Unknowingly, I ended up downloading the whole curated set of 2,331,700 small molecules (!), and I found this out when I loaded the dataframe after setting up the delimiter for the csv file, which later led to the file size problem mentioned earlier.

Loading these 2,331,700 rows of data was fast, which occurred within a few seconds without exaggeration. This echoed many users' experiences with Polars, so this was another nice surprise, and once again confirmed that Rust, and also Apache arrow, which was used as Polars' foundation, were solid in speed.

Now I had the full dataframe, and I wanted to find out what types of physicochemical properties were there for the compounds.

```{python}
# Print all column names and data types 
print(df.glimpse())
```

There were a few terms where I wasn't sure of their exact meanings, so I went through the ChEMBL_31 schema documentation and ChEMBL database website to find out. This took a while and was an important step to take so that I would know what to do when reaching the ML phase.

I have selected a few physicochemical properties down below so that readers and I could gather some reasonable understandings for each term. The explanations for each term were adapted from ChEMBL_31 schema documentation (available as "Release notes" on the website), or if definitions for certain terms were not available from the documentation, I resorted to interpret them myself by going into "Dinstict compounds" section on the ChEMBL database, where I would click on, e.g. bioactivities, for a random compound in there to see what results showed up and then described them below.

The definitions for some of the listed physicochemical properties were:

**Max Phase** - Maximum phase of development reached for the compound (where 4 = approved). Null was where max phase has not yet been assigned.

**Bioactivities** - Various biological assays used for the compounds e.g. IC~50~, GI~50~, potency tests etc.

**AlogP** - Calculated partition coefficient

**HBA** - Number of hydrogen bond acceptors

**HBD** - Number of hydrogen bond donors

**#RO5 Violations** - Number of violations of Lipinski's rule-of-five, using HBA and HBD definitions

**Passes Ro3** - Indicated whether the compound passed the rule-of-three (MW \< 300, logP \< 3 etc)

**QED Weighted** - Weighted quantitative estimate of drug likeness (as defined by Bickerton *et al.*, Nature Chem 2012)

**Inorganic flag** - Indicated whether the molecule was inorganic (i.e., containing only metal atoms and \<2 carbon atoms), where 1 = inorganic compound and -1 = not inorganic compound (assuming 0 meant it was neither case or yet to be assigned)

**Heavy Atoms** - Number of heavy (non-hydrogen) atoms

**CX Acidic pKa** - The most acidic pKa calculated using ChemAxon v17.29.0

**CX Basic pKa** - The most basic pKa calculated using ChemAxon v17.29.0

**CX LogP** - The calculated octanol/water partition coefficient using ChemAxon v17.29.0

**CX LogD** - The calculated octanol/water distribution coefficient at pH = 7.4 using ChemAxon v17.29.0

**Structure Type** - based on compound_structures table, where SEQ indicated an entry in the protein_therapeutics table instead, NONE indicated an entry in neither tables, e.g. structure unknown

**Inchi Key** - the IUPAC international chemical identifier key

From the df.glimpse() method previously, there were a lot of columns with the data type of "Utf8", which meant they were strings. There were only two columns that had "Int64", which meant they were integers. A lot of these columns were actually storing numbers as strings. So to make my life easier, I went on to convert these data types into the more appropriate ones for selected columns.

```{python}
# Convert data types for multiple selected columns
# Note: only takes two positional arguments, 
# so needed to use [] in code to allow more than two

# Multiple columns all at once - with_columns()
# *Single column - with_column() 
# *this only worked at the time of writing the post (around published date), 
# this is not going to work currently as Polars has been updated, 
# please use with_columns() for single or multiple columns instead*

# Use alias if wanting to keep original data type in column, 
# as it adds a new column under an alias name to dataframe
df_new = df.with_columns(
    [
        (pl.col("Molecular Weight")).cast(pl.Float64, strict = False),
        (pl.col("Targets")).cast(pl.Int64, strict = False),
        (pl.col("Bioactivities")).cast(pl.Int64, strict = False),
        (pl.col("AlogP")).cast(pl.Float64, strict = False),
        (pl.col("Polar Surface Area")).cast(pl.Float64, strict = False),
        (pl.col("HBA")).cast(pl.Int64, strict = False),
        (pl.col("HBD")).cast(pl.Int64, strict = False),
        (pl.col("#RO5 Violations")).cast(pl.Int64, strict = False),
        (pl.col("#Rotatable Bonds")).cast(pl.Int64, strict = False),
        (pl.col("QED Weighted")).cast(pl.Float64, strict = False),
        (pl.col("CX Acidic pKa")).cast(pl.Float64, strict = False),
        (pl.col("CX Basic pKa")).cast(pl.Float64, strict = False),
        (pl.col("CX LogP")).cast(pl.Float64, strict = False),
        (pl.col("CX LogD")).cast(pl.Float64, strict = False),
        (pl.col("Aromatic Rings")).cast(pl.Int64, strict = False),
        (pl.col("Heavy Atoms")).cast(pl.Int64, strict = False),
        (pl.col("HBA (Lipinski)")).cast(pl.Int64, strict = False),
        (pl.col("HBD (Lipinski)")).cast(pl.Int64, strict = False),
        (pl.col("#RO5 Violations (Lipinski)")).cast(pl.Int64, strict = False),
        (pl.col("Molecular Weight (Monoisotopic)")).cast(pl.Float64, strict = False)
    ]
)
df_new.head()
```

Once all the columns' data types have been checked and converted to appropriate types accordingly, I used null_count() to see the distributions of all null entries in the dataset.

```{python}
# Check for any null or NA or "" entries in the dataset
# Alternative code that worked similarly was df.select(pl.all().null_count())
df_new.null_count()
```

```{python}
# Drop rows with null entries
df_dn = df_new.drop_nulls()
df_dn 
# Number of rows reduced to 736,570
```

```{python}
# Check that all rows with null values were dropped
df_dn.null_count()
```

```{python}
# To see summary statistics for df_dn dataset
df_dn.describe()
```

<br>

##### **Some exploratory data analysis**

One of the columns that jumped out from the summary statistics of the df_dn dataset was the "Targets" column. It ranged from 1 to 1334 targets. Out of curiosity, I went through several places on ChEMBL website to find out the exact definition of "Target". Eventually I settled on an answer which explained that the "Target" column represented the number of targets associated with the particular ChEMBL compound listed. I then singled out the ChEMBL compound with 1334 targets recorded, it turned out to be imatinib, which was marketed as Gleevec, and was a well-known prescription medicine for leukaemia and other selected oncological disorders with many well-documented drug interactions.

```{python}
# This was confirmed via a filter function, which brought up CHEMBL1421, or also known as dasatinib
df_dn.filter(pl.col("Targets") == 1334)
```

To explore other physicochemical and molecular properties in the dataframe, "Max Phase" was one of the first few that drew my interests. So it tagged each ChEMBL compound with a max phase number from 0 to 4, where 4 meant the compound was approved (usually also meant it was already a prescription medicine). Thinking along this line, I thought what about those compounds that had max phase as 0, because they were the ones still pending associations with max phase numbers. By extending on this idea, this could be a good opportunity to introduce some ML to predict whether these zero max phase compounds would enter the approved max phase.

Firstly, I had a look at the overall distribution of the max phase compounds in this dataframe df_dn.

```{python}
# Interested in what types of "Max Phase" were recorded 
# for the curated small molecules in ChEMBL database
df_dn.groupby("Max Phase", maintain_order = True).agg(pl.count())
```

A quick groupby function showed that there were only 954 small molecules approved. Phase 3 recorded a total of 303 small molecules. For phase 2, there were 441 small molecules, followed by 239 compounds in phase 1. There were, however, a total amount of 734,633 small molecules that had zero as phase number (as per ChEMBL_31 schema documentation). Note: these figures were only for ChEMBL compounds with full documentations in the dataset (excluding entries or compounds with N/A or "" (empty) string cells).

One of the other parameters I was interested in was "QED Weighted". So I went further into understanding what it meant, as the original reference was conveniently provided in the ChEMBL_31 schema documentation. The reference paper was by [Bickerton, G., Paolini, G., Besnard, J. et al. Quantifying the chemical beauty of drugs. Nature Chem 4, 90--98 (2012)](https://doi.org/10.1038/nchem.1243) (note: author's manuscript was available to view via PubMed link, the Nature Chemistry link only provided abstract with access to article via other means as stated).

In short, it was a measure of druglikeness for small molecules based on the concept of desirability, which was based on a total of 8 different molecular properties. These molecular properties included molecular weight, ALogP, polar surface area, number of hydrogen bond acceptors, number of hydrogen bond donors, number of rotatable bonds, number of aromatic rings and structural alerts. Without going into too much details for this QED Weighted parameter, it was normally recorded as a number that ranged from 0 to 1, with 0 being the least druglike and 1 being the most druglike.

<br>

##### **Prepare dataframe prior to running machine learning model**

Before I got too carried away with further EDA, I wanted to get started on preparing a dataframe for the ML model. A rough plan at this stage was to filter out Max Phase 4 and 0 compounds. Max phase 0 compounds were the ones that were not assigned with any max phase numbers yet, so they would be ideal for use as the testing set. Another main idea was to use "Max Phase" parameter as the target y variable for a LR model, because ultimately stakeholders would be more interested in knowing which candidate compounds had the most likely chance to reach the final approved phase during a drug discovery and development project or otherwise. This would also provide a chance to potentially reduce the amount of resources and time required in such a complex and sophisticated matter.

The goal of this ML model was to answer this question: which physicochemical parameters would be the most suitable ones to predict whether a compound would enter max phase 4 (approved) or not? (implicitly, this might also help to predict which max phase 0 compounds would likely enter max phase 4 in the end)

I've then narrowed down the df_dn dataset to fulfill the following criteria:

-   Only small molecules present
-   Max phase of 0 and 4 only

Another reason behind choosing only small molecules that had max phase of 0 and 4 was that a confusion matrix could be built in the end to see if the parameters selected would give us a reasonably good model for predicting the outcomes of these small molecules.

For now, I've chosen the following columns (or physicochemical parameters) to appear in the interim df_0 and df_4 datasets.

```{python}
# Selecting Max phase 0 small molecules with desired parameters
df_0 = df_dn.filter(
    (pl.col("Type") == "Small molecule") &
    (pl.col("Max Phase") == 0)
).select(["ChEMBL ID", 
          "Type", 
          "Max Phase",
          "#RO5 Violations", 
          "QED Weighted", 
          "CX LogP", 
          "CX LogD", 
          "Heavy Atoms"]
        )
df_0
```

```{python}
# Selecting Max phase 4 small molecules with desired parameters
df_4 = df_dn.filter(
    (pl.col("Type") == "Small molecule") &
    (pl.col("Max Phase") == 4)
).select(["ChEMBL ID", 
          "Type", 
          "Max Phase",
          "#RO5 Violations", 
          "QED Weighted", 
          "CX LogP", 
          "CX LogD", 
          "Heavy Atoms"]
        )
df_4
```

<br>

###### **Re-sampling via under-sampling**

Because of the large number of Max Phase 0 compounds present in the original dataset, I've randomly sampled about 950 small molecules from this group, so that there were similar amount of data in each group to avoid having an imbalanced dataset.

```{python}
df_s_0 = df_0.sample(n = 950, shuffle = True, seed = 0)
df_s_0
```

Since the plan was to use LR method for ML model, the y variable I was interested in was going to be a binary categorical variable - meaning it needed to be 0 (not approved) or 1 (approved). To do this, I've added a new column with a new name of "Max_Phase" and replace "4" as "1" by dividing the whole column by 4 to reach this new label.

```{python}
df_4_f = df_4.with_columns((pl.col("Max Phase") / 4).alias("Max_Phase"))
df_4_f
```

Then I changed the data type of "Max_Phase" from float to integer, so that the two different dataframes could be concatenated (which would only work if both were of same data types).

```{python}
df_4_f = df_4_f.with_columns((pl.col("Max_Phase")).cast(pl.Int64, strict = False))
df_4_f
```

Also I've created a new column with the same name of "Max_Phase" for Max phase 0 small molecules, so that the two dataframes could be combined (also needed to have exactly the same column names for it to work).

```{python}
df_s_0_f = df_s_0.with_columns((pl.col("Max Phase")).alias("Max_Phase"))
df_s_0_f
```

Then I combined df_s\_0_f (dataframe with max phase 0 compounds) and df_4\_f (dataframe with max phase 4 compounds).

```{python}
df_concat = pl.concat([df_s_0_f, df_4_f], how = "vertical",)
print(df_concat)
```

This df_concat dataset was checked to see it had all compounds in Max Phase 0 and 4 only. Note: Max Phase 4 (approved) compounds were re-labelled as Max_Phase = 1.

```{python}
df_concat.groupby("Max_Phase").count()
```

I then checked df_concat dataset only had small molecules to confirm what I've tried to achieve.

```{python}
df_concat.groupby("Type").count()
```

So here we had the final version of the dataset, which I've renamed to df_ml to avoid confusion from the previous dataframes, before entering the ML phase.

```{python}
# Leave out ChEMBL ID and Type
df_ml = df_concat.select(["Max_Phase", 
                          "#RO5 Violations", 
                          "QED Weighted", 
                          "CX LogP", 
                          "CX LogD", 
                          "Heavy Atoms"]
                        )
df_ml
```

```{python}
# Check for any nulls in the dataset
df_ml.null_count()
```

```{python}
# Check data types in df_ml dataset
# Needed to be integers or floats for scikit-learn algorithms to work
df_ml.dtypes
```

``` {{python}}
# Note: exported df_ml dataframe as csv file for ML series 1.2.
df_ml.write_csv("df_ml.csv", sep = ",")
```

<br>

##### **Import libraries for machine learning**

```{python}
# Install scikit-learn - an open-source ML library
# Uncomment the line below if needing to install this library
#!pip install -U scikit-learn
```

```{python}
# Import scikit-learn
import sklearn

# Check version of scikit-learn 
print(sklearn.__version__)
```

Other libraries needed to generate ML model were imported as below.

```{python}
# To use NumPy arrays to prepare X & y variables
import numpy as np

# Needed for dataframe in scikit-learn ML
# Uncomment line below if requiring to install pandas
#!pip install pandas
import pandas as pd

# To normalise dataset prior to running ML
from sklearn import preprocessing
# To split dataset into training & testing sets
from sklearn.model_selection import train_test_split

# For data visualisations
# Uncomment line below if requiring to install matplotlib
#!pip install matplotlib
import matplotlib.pyplot as plt
```

I've then installed pyarrow, to convert Polars dataframe into a Pandas dataframe, which was needed to run *scikit-learn*.

```{python}
# Uncomment line below to install pyarrow
#!pip install pyarrow
```

```{python}
# Convert Polars df to Pandas df 
df_ml_pd = df_ml.to_pandas()
type(df_ml_pd)
```

<br>

##### **Logistic regression with *scikit-learn***

LR was one of the supervised methods in statistical ML realm. As the term "supervised" suggested, this type of ML was purely data-driven to allow computers to learn patterns from input data with known outcomes, in order to predict new outcomes on novel data.

<br>

###### **Defining X and y variables**

```{python}
# Define X variables from df_ml_pd dataset
X = np.asarray(df_ml_pd[["#RO5 Violations", 
                         "QED Weighted", 
                         "CX LogP", 
                         "CX LogD", 
                         "Heavy Atoms"]]
              )
X[0:5]
```

```{python}
# Define y variable
# Note to use "Max_Phase", not the original "Max Phase"
y = np.asarray(df_ml_pd["Max_Phase"])
y[0:5]
```

<br>

###### **Training and testing sets**

```{python}
# Split dataset into training & testing sets

# Random number generator
#rng = np.random.RandomState(0) - note: this may produce different result each time

# Edited post to use random_state = 250 to show comparison with ML series 1.2
# for reproducible result
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 250)
print('Training set:', X_train.shape, y_train.shape)
print('Testing set:', X_test.shape, y_test.shape)
```

<br>

###### **Preprocessing data**

```{python}
# Normalise & clean the dataset
# Fit on the training set - not on testing set as this might lead to data leakage
# Transform on the testing set
X = preprocessing.StandardScaler().fit(X_train).transform(X_test)
X[0:5]
```

<br>

###### **Fitting LR classifier on training set**

```{python}
# Import logistic regression 
from sklearn.linear_model import LogisticRegression
# Create an instance of logistic regression classifier and fit the data
LogR = LogisticRegression().fit(X_train, y_train)
LogR
```

<br>

###### **Applying LR classifier on testing set for prediction**

```{python}
y_mp = LogR.predict(X_test)
y_mp
```

<br>

###### **Converting predicted values into a dataframe**

```{python}
# Predicted values were based on log odds
# Use describe() method to get characteristics of the distribution
pred = pd.DataFrame(LogR.predict_log_proba(X))
pred.describe()
```

Alternatively, a quicker way to get predicted probabilities was via predict_proba() method in *scikit-learn*.

```{python}
y_mp_proba = LogR.predict_proba(X_test)
# Uncomment below to see the predicted probabilities printed
#print(y_mp_proba)
```

<br>

###### **Converting predicted probabilities into a dataframe**

```{python}
# Use describe() to show distributions
y_mp_prob = pd.DataFrame(y_mp_proba)
y_mp_prob.describe()
```

<br>

###### **Pipeline method for LR**

This was something I thought to try when I was reading through *scikit-learn* documentation. One major advantage of using pipeline was that it was designed to chain all the estimators used for ML. The benefit of this was that we only had to call fit and predict once in our data to fit the whole chain of estimators. The other useful thing was that this could avoid data leakage from our testing set into the training set by making sure the same set of samples were used to train the transformers and predictors. One other key thing it also helped was that it also avoided the possibility of missing out on the transformation step.

The example below used the function of make_pipeline, which took in a number of estimators as inputted, and then constructed a pipeline based on them.

```{python}
# Test pipline from scikit-Learn
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

LR = make_pipeline(StandardScaler(), LogisticRegression())
LR.fit(X_train, y_train)
```

<br>

#### **Evaluation of the logistic regression model**

###### **Accuracy scores**

```{python}
from sklearn.metrics import accuracy_score
accuracy_score(y_mp, y_test)
```

The accuracy score was 0.7 (after rounding up) based on the original data preprocessing method, which meant that there were around 70% of the cases (or compounds) classified correctly by using this LR classifier. Accuracy literally provided a measure of how close the predicted samples were to the true values. One caveat to note was that for imbalanced dataset, accuracy score might not be very informative, and other evaluation metrics would need to be considered instead.

The accuracy score shown below was from the pipeline method used previously, which showed a very similar accuracy score of 0.69656992 (close to 0.7), confirming the method was in line with the original preprocessing method.

```{python}
LR.score(X_test, y_test)
```

<br>

###### **Confusion matrix**

Next, I've built a confusion matrix based on the model in order to visualise the counts of correct and incorrect predictions. The function code used below was adapted from the IBM data science course I've taken around the end of last year. I've added comments to try and explain what each section of the code meant.

```{python}
# Import confusion matrix from scikit-learn
from sklearn.metrics import confusion_matrix
# Import itertools - functions to create iterators for efficient looping
import itertools

# Function to print and plot confusion matrix
def plot_confusion_matrix(# Sets a cm object (cm = confusion matrix)
                          cm, 
                          # Sets classes of '1s' (Successes) & '0s' (Non-successes) for the cm
                          classes,
                          # If setting normalize = true, reports in ratios instead of counts
                          normalize,
                          title = 'Confusion matrix',
                          # Choose colour of the cm (using colourmap recognised by matplotlib)
                          cmap = plt.cm.Reds):
    
    if normalize:
        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    # Plot the confusion matrix 
    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 45)
    plt.yticks(tick_marks, classes)

    # Floats to be round up to two decimal places if using normalize = True
    # or else use integers
    fmt = '.2f' if normalize else 'd'
    # Sets threshold of 0.5
    thresh = cm.max() / 2.
    # Iterate through the results and differentiate between two text colours 
    # by using the threshold as a cut-off
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment = "center",
                 color = "white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
```

```{python}
# Compute confusion matrix
matrix = confusion_matrix(y_test, y_mp, labels = [0,1])
np.set_printoptions(precision = 2)

# Plot confusion matrix without normalisation
plt.figure()
plot_confusion_matrix(matrix, 
                      # Define classes of outcomes
                      classes = ['Max_Phase = 0','Max_Phase = 1'], 
                      # Set normalize = True if wanting ratios instead
                      normalize = False, 
                      title = "Confusion matrix without normalisation"
                     )
```

A common rule of thumb for confusion matrix was that all predicted outcomes were columns and all the true outcomes were rows. However, there might be exceptions where this was the other way round. Four different categories could be seen in the confusion matrix which were:

-   True positive - Predicted Max_Phase = 1 & True Max_Phase = 1 (126 out of 189 samples)
-   True negative - Predicted Max_Phase = 0 & True Max_Phase = 0 (139 out of 190 samples)
-   False positive - Predicted Max_Phase = 1 & True Max_Phase = 0 (63 out of 189 samples)
-   False negative - Predicted Max_Phase = 0 & True Max_Phase = 1 (51 out of 190 samples)

By having these four categories known would then lead us to the next section about classification report, which showed all the precision, recall, f1-score and support metrics to evaluate the performance of this classifier.

<br>

###### **Classification report**

```{python}
from sklearn.metrics import classification_report
print(classification_report(y_test, y_mp))
```

***Precision*** was a measure of the accuracy of a predicted outcome, where a class label had been predicted by the classifier. So in this case, we could see that for class label 1, the precision was 0.67, which corresponded to the true positive result of 126 out of 189 samples (= 0.666). It was defined by:

$$
\text{Precision} = \frac{\Sigma\ True\ Positive}{(\Sigma\ True\ Positive + \Sigma\ False\ Positive)}
$$

***Recall***, also known as sensitivity (especially widely used in biostatistics and medical diagnostic fields), was a measure of the strength of the classifier to predict a positive outcome. In simple words, it measured the true positive rate. In this example, there was a total of 126 out of 177 samples (which = 0.712, for True Max_Phase = 1 row) that had a true positive outcome of having a max phase of 1. It was defined by:

$$
\text{Recall} = \frac{\Sigma\ True\ Positive}{(\Sigma\ True\ Positive + \Sigma\ False\ Negative)}
$$

The precision and recall metrics could also be calculated for class label = 0, which were shown for the row 0 in the classification report.

***f1-score***, or also known as balanced F-score or F-measure, denoted the harmonic average of both precision and recall metrics. This metric would also give another indication about whether this model performed well on outcome predictions. It normally ranged from 0 (worst precision and recall) to 1 (perfect precision and recall). For this particular classifier, f1-score was at 0.69 (for class label = 1), which was definitely not at its worst, but also could be further improved. It was defined as:

$$
\text{F1-score} = \frac{2 \times (Precision \times Recall)}{(Precision + Recall)}
$$

***Support***, which some readers might have already worked out how the numbers were derived, was the total number of true samples in each class label (reading row-wise from the confusion matrix). The main purpose of showing this metric was to help clarifying whether the model or classifier had a reasonably balanced dataset for each class or otherwise.

<br>

###### **Log loss**

Log loss could be used as another gauge to show how good the classifier was at making the outcome predictions. The further the predicted probability was from the true value, the larger the log loss, which was also ranged from 0 to 1. Ideally, the smaller the log loss the better the model would be. Here, we had a log loss of 0.607 for this particular model.

```{python}
# Log loss
from sklearn.metrics import log_loss
log_loss(y_test, y_mp_proba)
```

#### **Discussions and conclusion**

So here I've completed a very basic LR classifier model for ChEMBL compound dataset. By no means was this a perfect ML model as I haven't actually changed the default settings of *scikit-learn*'s LogisticRegression() classifier, with examples such as adjusting C, a regularization parameter which was set at '1.0' by default, and also solvers, which could take in different algorithms for use in optimisation problems and normally set as 'lbfgs' by default.

So with this default LR model, the evaluation metrics demonstrated a LR classifer of moderate quality to predict the approval outcomes on ChEMBL small molecules, with a lot of rooms for improvements. Therefore, I could not yet confirm fully that the physicochemical parameters chosen would be the best ones to predict the approval outcomes for any small molecules. However, I might be okay to say that these molecular parameters were on the right track to help with making this prediction.

To further improve this model, I could possibly trial changing the C value and use different solvers to see if better outcomes could be achieved, or even add more molecular parameters in the model to test. I could have also trialled adding more class labels, e.g. making it between max phase 1, 2 and 4, or a mix-and-match between each max phase category. Other things to consider would be to use other types of ML methods such as naive Bayes, K-nearest neighbours or decision trees and so on. To tackle the problem thoroughly, I would most likely need to do an ensemble of different ML models to find out which model would be the most optimal to answer our target question.

<br>

#### **Final words**

I've experienced the fun of ML after completing this project. The idea was to build on what I knew gradually and enjoy what ML could do when making critical decisions. From what I've learnt about ML so far (and definitely more to learn) was that the quality of data was vital for making meaningful interpretations of the results.

However, jumping back to present time, I'll need to work on my second project first, which is about using Rust interactively via Jupyter notebook. At the moment, I'm not sure how long it will take or how the content will play out. I'll certainly do as much as I can since Rust is very new to me. If I get very stuck, I'd most likely continue on this ML series. Thanks for reading.

<br>

#### **References**

I've listed below most of the references used throughout this project. Again, huge thanks could not be forgotten for our online communities, and definitely also towards the references I've used here.

-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html)
-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.
-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.
-   [Stack Overflow](https://stackoverflow.com)
-   Polars references:
    1.  [Polars - User Guide](https://pola-rs.github.io/polars-book/user-guide/introduction.html) - https://pola-rs.github.io/polars-book/user-guide/introduction.html
    2.  [Polars documentation](https://pola-rs.github.io/polars/py-polars/html/index.html#) - https://pola-rs.github.io/polars/py-polars/html/index.html#
    3.  [Polars GitHub repository](https://github.com/pola-rs/polars) - https://github.com/pola-rs/polars
