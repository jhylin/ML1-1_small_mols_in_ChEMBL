---
title: Small molecules in ChEMBL database
subtitle: Series 1.1.2 - Preprocessing data in Polars dataframe library
author: Jennifer HY Lin
date: '2023-1-4'
date-modified: last-modified
draft: true
categories:
  - Machine learning projects
  - Polars
  - Python
  - Jupyter
  - ChEMBL database
  - Cheminformatics
jupyter: python3
bibliography: references.bib
---

##### **Import Polars and read parquet file**

This second post will mostly be about doing some data preprocessing for the parquet file we've saved from the first post, trying to get the data in a reasonable state before moving onto the machine learning model building and training phase. The first step is to import Polars dataframe library and then read the parquet file as a dataframe.

```{python}
import polars as pl
df_pa = pl.read_parquet("chembl_sm_mols.parquet")
df_pa
```

I'm having a look at the max phase column first to see the distribution or counts of molecules in each max phase.

```{python}
df_pa.group_by("Max Phase").len()
```

I also want to find out what types of physicochemical properties are there in this dataset.

```{python}
# Print all column names and data types 
print(df_pa.glimpse())
```

<br>

###### **Looking at some of the physicochemical properties in the dataset**

I've gone through the ChEMBL_31 schema documentation and ChEMBL database website to find out the meanings for some of the column names. The explanations of the selected term are adapted from ChEMBL_31 schema documentation (available as "Release notes" on the website at the time), or if definitions for certain terms are not available, I resort to interpret them myself by going into "Dinstict compounds" section of the ChEMBL database.

The definitions for some of the listed physicochemical properties are:

**Max Phase** - Maximum phase of development reached for the compound (where 4 = approved). Null is where max phase has not yet been assigned.

**Bioactivities** - Various biological assays used for the compounds e.g. IC~50~, GI~50~, potency tests etc.

**AlogP** - Calculated partition coefficient

**HBA** - Number of hydrogen bond acceptors

**HBD** - Number of hydrogen bond donors

**#RO5 Violations** - Number of violations of Lipinski's rule-of-five, using HBA and HBD definitions

**Passes Ro3** - Indicating whether the compound passed the rule-of-three (MW \< 300, logP \< 3 etc)

**QED Weighted** - Weighted quantitative estimate of drug likeness [@Bickerton2012]

**Inorganic flag** - Indicating whether the molecule is inorganic (i.e., containing only metal atoms and \<2 carbon atoms), where 1 = inorganic compound and 0 = non-inorganic compound (-1 meaning preclinical compound or not a drug)

**Heavy Atoms** - Number of heavy (non-hydrogen) atoms

**CX Acidic pKa** - The most acidic pKa calculated using ChemAxon v17.29.0

**CX Basic pKa** - The most basic pKa calculated using ChemAxon v17.29.0

**CX LogP** - The calculated octanol/water partition coefficient using ChemAxon v17.29.0

**CX LogD** - The calculated octanol/water distribution coefficient at pH = 7.4 using ChemAxon v17.29.0

**Structure Type** - based on compound_structures table, where SEQ indicates an entry in the protein_therapeutics table instead, NONE indicates an entry in neither tables, e.g. structure unknown

**Inchi Key** - the IUPAC international chemical identifier key

The older version of this post has had a code section for changing the data types of several columns. I've found out that this is no longer needed as I've taken care of this in the first post when I've specifically stated the null_values (e.g. "None" and "") to be converted to"null" during `pl.read_csv()`. When the same dataframe df_pa is read here, you'll notice that the data type for each column should be how it should be, matching the data inside each column.

<br>

##### **Dealing with nulls**

I'm using `null_count()` to see the distributions of all null entries in the dataset.

```{python}
# Alternative code: df_pa.select(pl.all().null_count())
df_pa.null_count()
```

The following is basically several different ways to remove null values from the original dataset.

```{python}
## ---Drop all rows with null entries---
#df_pa.drop_nulls()

## Number of rows reduced to 2,645 - seems way too much...

## ---Restricting drop nulls to only subsets of nulls in strings ---
# import polars.selectors as cs
# df_pa.drop_nulls(subset=cs.string())

## Number of rows reduced to 17,735 - hmm... try other ways?

## ---Drop a row if all values are null---
# df_pa.filter(~pl.all_horizontal(pl.all().is_null()))

## No change in number of rows - meaning no one row contains all null values

## ---Restricting drop nulls to a specific column---
# df_pa.drop_nulls(subset="CX LogP")

## Number of rows reduced to 1,873,678
```

Initially, I've tried to remove all nulls first, then I realise that removing nulls above may not be the best way to prepare the data as all max phase 4 compounds are actually also removed completely! So what I'm going to do instead is to keep them by using `fill_null()` to replace all "null" as "0" (this'll only apply to all the integers or floats in the data).

```{python}
df_pa = df_pa.fill_null(0)
df_pa
```

```{python}
# Summary statistics for df_dn dataset
df_pa.describe()
```

<br>

##### **Looking at max phase and QED weighted**

To explore some of the physicochemical and molecular properties in the dataframe, "Max Phase" is one of the first few that I want to have a look. Each ChEMBL compound will have a max phase number from 0 to 4 (for this particular dataset only, you'll notice that this can start from -1 in other ChEMBL datasets), where 4 means the compound is approved (e.g. a prescription medicine).

While looking at max phases, I'm thinking the compounds with max phase 0 could be a testing set for prediction of their max phase outcomes, while the max phase 4 compounds could be the training set for building a machine learning (ML) model. There are of course some obvious and potential flaws of splitting a dataset like this, please treat this as an exercise for demonstration only.

Below I'm checking that I do have max phases of molecules spanning across the entire range of 0 to 4.

```{python}
## Alternative code
# df_pa.group_by("Max Phase", maintain_order = True).agg(pl.len())
df_pa.group_by("Max Phase").len()
```

One of the other parameters I'm interested in is "QED Weighted" [@Bickerton2012]. It's a measure of druglikeness for small molecules based on the concept of desirability, which is based on a total of 8 different molecular properties. These molecular properties include molecular weight, ALogP, polar surface area, number of hydrogen bond acceptors, number of hydrogen bond donors, number of rotatable bonds, number of aromatic rings and structural alerts. It's normally recorded as a number ranging from 0 to 1, where 0 is the least druglike and 1 being the most druglike.

<br>

##### **Prepare data prior to running ML model**

The goal of this ML model is to answer this question - which physicochemical features will be useful to predict whether a compound will enter max phase 4? 

A rough plan at this stage is:

- to filter out max phase 4 and 0 compounds from the df_pa dataset

- to use "Max Phase" parameter as the target y variable for a logistic regression (LR) model because ultimately stakeholders are more likely to be interested in knowing which candidate compounds have the most likely chance to reach the final approved phase during a drug discovery project (*please bear in mind that this is of course way too simplified and not reflecting real-life scenarios...*)

- to use "Polar Surface Area", "HBA", "HBD", "#RO5 Violations", "QED Weighted", "CX LogP", "CX LogD" and "Heavy atoms" as the training features for building the ML model (*these molecular features are selected randomly and ideally I should include more... the first three are newly added in this updated version*)

- to use max phase 0 compounds as the testing set since they are the ones not assigned with any max phase category yet

- to use max phase 4 compounds as the training set since they've reached the approved phase

- to build a confusion matrix in the end to see if the features selected may generate a somewhat reasonable model for predicting the outcomes of these small molecules

Downsides of this plan are (there are many...):

- the dataset used here will include some inorganic compounds (e.g. molecules containing metals)

- the dataset is likely not very-chemically-diverse 

- there are only a small number of max phase 4 compounds for model training (especially comparing with the "much larger" amount of max phase 0 compounds present)

- LR may not be the best choice of ML model to start off with (but I just want to find out how it works at the time and hence I'm doing it here)

- other exploratory data analysis should be done really... e.g. how chemically diverse the dataset is?

- this is only an exercise or demonstration-like piece so will not be taking into account of many other important aspects of drug discovery e.g. what disorders or illnesses are we targeting? (is it an experimentally validated and suitable target?) and what have been done so far in the literatures? etc.

Firstly, I'm filtering out all the max phase 0 compounds and also all the max phase 4 compounds then save them in separate dataframes.

```{python}
# Selecting all max phase 0 molecules with their training features
df_0 = df_pa.filter((pl.col("Max Phase") == 0)).select([
  "ChEMBL ID", 
  "Max Phase",
  "Polar Surface Area",
  "HBA",
  "HBD",
  "#RO5 Violations", 
  "QED Weighted", 
  "CX LogP", 
  "CX LogD", 
  "Heavy Atoms"
  ])
df_0  #1,826,345 mols
```

```{python}
# Selecting all max phase 4 molecules with their training features
df_4 = df_pa.filter((pl.col("Max Phase") == 4)).select([
      "ChEMBL ID", 
      "Max Phase",
      "Polar Surface Area",
      "HBA",
      "HBD",
      "#RO5 Violations", 
      "QED Weighted", 
      "CX LogP", 
      "CX LogD", 
      "Heavy Atoms"
      ])
df_4  # 2,870 mols
```

```{python}
df_4.describe()
```

I have to go back to the first post at this point to change the data restriction criteria for the parquet file as I've found out in this step above that if I restrict data via inorganic flags, then it'll remove a lot of compounds with calculated physicochemical properties (because most of the ones in the data are preclinical compounds with inorganic flag of -1 or max phase 0 compounds; inorganic flag of 1 is inorganic ones and 0 should be the non-inorganic ones - I've updated its definition above too after re-checking the ChEMBL schema). For now, I'm restricting data via removing compounds with zero targets rather than via the inorganic flag, and this leaves us a set of max phase 4 compounds with their physicochemical properties calculated in ChEMBL (which is needed for training the model in the next post).

<br>

###### **Re-sampling via under-sampling - dealing with imbalanced dataset**

Because of the large number of max phase 0 compounds (1,826,345 molecules) and the relatively smaller number of max phase 4 compounds (2,870 molecules) present in the original dataset, I'm going to randomly sample 2800 small molecules from the max phase 0 group to balance the ratio of max phase 0 versus max phase 4 compounds. This will allow us to have a similar amount of data in each group to avoid a very imbalanced dataset.

::: {.callout-note}
For an alternative and likely a better way to deal with imbalanced datasets - please see [my later post](https://jhylin.github.io/Data_in_life_blog/posts/17_ML2-2_Random_forest/2_random_forest_classifier.html#model-building)  that attempts to use a "generalized threshold shifting" method.
:::

```{python}
df_0 = df_0.sample(n = 2800, shuffle = True, seed = 0)
df_0
```

Since the plan is to use LR method for the ML model, the y variable I'm interested in is going to be a binary categorical variable - meaning it needs to be 0 (not approved) or 1 (approved). To do this, I'm going to add a new column with a new name of "Max_Phase" and replace "4" as "1" by dividing the whole column by 4 to reach this new label in the max phase 4 dataset.

Then I'm changing the data type of "Max_Phase" from float to integer, so that the two different dataframes can be concatenated (which will only work if both are of the same data types).

```{python}
df_4 = df_4.with_columns((pl.col("Max Phase") / 4).alias("Max_Phase")).cast({"Max_Phase": pl.Int64})
df_4
```

I'm also going to create a new column with the same name of "Max_Phase" in the max phase 0 dataframe, so that the two dataframes can be combined.

```{python}
df_0 = df_0.with_columns((pl.col("Max Phase")).alias("Max_Phase"))
df_0
```

Then I'm combining df_0 (dataframe with max phase 0 compounds) and df_4 (dataframe with max phase 4 compounds).

```{python}
df_concat = pl.concat([df_0, df_4])
df_concat
```

This df_concat dataset is checked to see if it has all compounds in max phases 0 and 4 only. 

Note: max phase 4 (approved) compounds are re-labelled as Max_Phase = 1.

```{python}
df_concat.group_by("Max_Phase").len()
```

So here we have the final version of the dataset, renamed to df_ml to avoid confusion from the previous dataframes, before entering the ML phase.

```{python}
df_ml = df_concat.select([
  "Max_Phase", 
  "Polar Surface Area",
  "HBA",
  "HBD",
  "#RO5 Violations", 
  "QED Weighted", 
  "CX LogP", 
  "CX LogD", 
  "Heavy Atoms"
  ])
df_ml
```

I'm just checking for any nulls in the final dataset (should be none as I've filled all nulls in one of the steps above).

```{python}
df_ml.null_count()
```

Also, I'm checking the data types in df_ml dataset to make sure they're all integers or floats for Scikit-learn algorithms to work.

```{python}
# also shown in the dataframe below the column names
df_ml.dtypes
```

Then finally I'm saving this tidied dataset as a separate .csv file for use in the next post.

``` {python}
df_ml.write_csv("df_ml.csv", separator = ",")
```
